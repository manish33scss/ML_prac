# -*- coding: utf-8 -*-
"""
Created on Thu May 28 02:49:25 2020

@author: manish kumar
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

import os
import numpy as np
import  cv2
from tqdm import tqdm

REBUILD_DATA = True

#check cuda
torch.cuda.is_available()

device = torch.device("cuda:0")

if torch.cuda.is_available():
    device = torch.device("cuda:0")
else:
    device = torch.device("cpu")
    print("device running on CPU")
        
class Dogvscats():
    IMG_SIZE = 50
    CATS = r"D:\Work\Data_files\data\New folder\PetImages\Cat"
    DOGS = r"D:\Work\Data_files\data\New folder\PetImages\Dog"
    LABELS = {CATS: 0, DOGS: 1}
    
    training_data = []
    catcount = 0
    dogcount = 0
    
    def make_train_data(self):
        for label in self.LABELS:
            print(label)
            for f in tqdm(os.listdir(label)):
                try:
                    path = os.path.join(label, f)
                    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
                    img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))
                    self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])
                    
                    if label == self.CATS:
                        self.catcount += 1
                    elif label == self.DOGS:
                        self.dogcount +=1
                    
                except Exception as e:
                    pass
                
        np.random.shuffle(self.training_data)
        np.save("Training_Data.npy", self.training_data)
        print("Cats", self.catcount)
        print("dogs", self.dogcount)
        
      
if REBUILD_DATA:
    dvsc = Dogvscats()
    dvsc.make_train_data()
    
    
training_data = np.load("Training_Data.npy", allow_pickle = True)
print(len(training_data))



#BUILDING MODEL



class Net(nn.Module):
    def __init__(self):
        super().__init__()
        
        #lets create layers
        
        self.conv1= nn.Conv2d(1,32, 5)  #input , output () features, kernel (5*5 iwindow)
        self.conv2= nn.Conv2d(32,64, 5)
        self.conv3= nn.Conv2d(64,128, 5)
        
        #flatten
        x = torch.randn(50,50).view(-1,1,50,50)
        self._to_linear = None
        self.convs(x)
        
        self.fc1 = nn.Linear(self._to_linear , 512)
        self.fc2 = nn.Linear(512,2)
        
    def convs(self, x):
        
        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))
        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))
        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))
        
        if self._to_linear is None:
            self._to_linear = x[0].shape[0]* x[0].shape[1] * x[0].shape[2]
            
        return x
    
    def forward(self, x):
        x = self.convs(x)
        x = x.view(-1,self._to_linear) #now flattened
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        
        return F.softmax(x, dim =1)
    
net = Net().to(device)
print(net)
import torch.optim as optim

optimizer = optim.Adam(net.parameters(), lr = 0.001)
loss_function = nn.MSELoss()

X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)
X = X/255 #pixel scaling
y = torch.Tensor([i[1] for i in training_data])

val_per = 0.1
val_size = int(len(X)*val_per)



train_X = X[:-val_size]
train_y = y[:-val_size]

test_X = X[-val_size:]
test_y = y[-val_size:]

print(len(train_X))
print(len(test_X))

Batch_size = 100
EPOCHS = 6
def train(net):
    for epoch in range(EPOCHS):
        for i in tqdm(range(0, len(train_X), Batch_size)):
            #print(i, i+Batch_size)
            batch_X = train_X[i:i+Batch_size].view(-1,1,50,50).to(device)
            
            batch_y = train_y[i:i+Batch_size].to(device)
            
            
            net.zero_grad()
            outputs = net(batch_X)
            loss = loss_function(outputs, batch_y)
            loss.backward()
            optimizer.step()
        print("Epoch : "+str(epoch), "loss : "+str(loss))
        
            
    
        
#predict
def test(net):        
    correct = 0
    total = 0
    with torch.no_grad():
        for i in tqdm(range(len(test_X))):
            real_class = torch.argmax(test_y[i]).to(device)
            net_out = net(test_X[i].view(-1,1,50,50).to(device))[0]
            
            predicted_class = torch.argmax(net_out)
            if predicted_class == real_class:
                correct += 1
            total += 1
            
    print ("ACc", round(correct/total, 3))
                    
train(net)                
test(net)    

def fwd_pass(X,y,train=False):
    if train:
        net.zero_grad()
    outputs = net(X)
    matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs , y)]
    acc = matches.count(True)/ len(matches)
    loss = loss_function(outputs, y)
    
    if train:
        loss.backward()
        optimizer.step()
    return acc, loss

def test(size = 32):
    random_start = np.random.randint(len(test_X)-size)
    X, y = test_X[:size], test_y[:size]
    with torch.no_grad():
        val_acc, val_loss = fwd_pass(X.view(-1,-1,50,50).to(device), y.to(device))
    return val_acc, val_loss
